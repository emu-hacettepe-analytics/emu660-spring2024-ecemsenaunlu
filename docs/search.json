[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! My name is Ecem Sena Unlu\nThis is my personal webpage..\nPlease stay tuned to follow my works on data analytics, blog posts, and more."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project Distribution of education levels by provinces and gender in Turkey over the years",
    "section": "",
    "text": "Welcome to my project page.\nKeep an eye on this space to stay updated with my project activities.\n(The titles below are provided as examples; please feel free to adjust them as necessary.)"
  },
  {
    "objectID": "project.html#data-source",
    "href": "project.html#data-source",
    "title": "Project Distribution of education levels by provinces and gender in Turkey over the years",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\nCompleted education by province (15+ years old)\n2020 Data set of Convicts Entered into Penal Institutions by Crime Type and Educational Level, crime situations in the province\nYou can access following datas from the link below:\nReach Out To Data Source"
  },
  {
    "objectID": "project.html#general-information-about-data",
    "href": "project.html#general-information-about-data",
    "title": "Project Distribution of education levels by provinces and gender in Turkey over the years",
    "section": "2.2 General Information About Data",
    "text": "2.2 General Information About Data\nBelow you can find the names of all columns in the Education and Crime datasets and information about the data they represent.\nYear : The Year where data is collected for statistical calculations.\nProvinceID : The ProvinceID where data is collected for statistical calculations.\nProvinceName : The ProvinceName where data is collected for statistical calculations.\nilliterateTotal : The illiterateTotal where Total number of illiterate people in Turkey.\nilliterateMale : The IlliterateMale where Total number of illiterate men in Turkey.\nilliterateFemale : The IlliterateFemale where Total number of illiterate women in Turkey.\nLiterateWithoutADiplomaTotal :The LiterateWithoutADiplomaTotal is total number of people in Turkey which has no diploma.\nLiterateWithoutADiplomaMale :The LiterateWithoutADiplomaTotal is total number of men in Turkey which has no diploma.\nLiterateWithoutADiplomaFemale :The LiterateWithoutADiplomaTotal is total number of women in Turkey which has no diploma.\nPrimarySchoolTotal : The PrimarySchoolTotal is number of people which graduated from Primary school in Turkey.\nPrimarySchoolMale : The PrimarySchoolMale is number of men which graduated from Primary school in Turkey.\nPrimarySchoolFemale : The PrimarySchoolMale is number of women which graduated from Primary school in Turkey.\nPrimaryEducationTotal : The PrimaryEducationTotal is number of people which graduated from Primary Education in Turkey. \nPrimaryEducationMale : The PrimaryEducationMale is number of men which graduated from Primary Education in Turkey.\nPrimaryEducationFemale : The PrimaryEducationFemale is number of women which graduated from Primary Education in Turkey.\nLowerSecondarySchoolTotal : The LowerSecondarySchoolTotal is number of people which graduated from Lower Secondary School in Turkey.\nLowerSecondarySchoolMale :The LowerSecondarySchoolMale is number of men which graduated from Lower Secondary School in Turkey.\nLowerSecondarySchoolFemale :The LowerSecondarySchoolFemale is number of women which graduated from Lower Secondary School in Turkey.\nUpperSecondarySchoolTotal :The UpperSecondarySchoolTotal is number of people which graduated from Upper Secondary School in Turkey.\nUpperSecondarySchoolMale :The LowerSecondarySchoolMale is number of men which graduated from Upper Secondary School in Turkey.\nUpperSecondarySchoolFemale :The LowerSecondarySchoolFemale is number of women which graduated from Upper Secondary School in Turkey.\nHigherEducationalinstitutionsTotal :The HigherEducationalinstitutionsTotal is number of people which graduated from Higher Educational institutions in Turkey.\nHigherEducationalinstitutionsMale :The HigherEducationalinstitutionsMale is number of men which graduated from Higher Educational institutions in Turkey.\nHigherEducationalinstitutionsFemale :The HigherEducationalinstitutionsFemale is number of women which graduated from Higher Educational institutions in Turkey.\nMasterTotal :The MasterTotal is number of people which graduated from Master in Turkey.\nMasterMale : The MasterTotal is number of men which graduated from Master in Turkey.\nMasterFemale : The MasterTotal is number of women which graduated from Master in Turkey.\nDoctorateTotal : The DoctorateTotal is number of people which graduated from Master in Turkey.\nDoctorateMale :The DoctorateTotal is number of men which graduated from Master in Turkey.\nDoctorateFemale :The DoctorateTotal is number of women which graduated from Master in Turkey.\nUnknownTotal : The UnknownTotal is the number of people whose educational status is unknown in Turkey.\nUnknownMale :The UnknownTotal is the number of men whose educational status is unknown in Turkey.\nUnknownFemale :The UnknownTotal is the number of women whose educational status is unknown in Turkey."
  },
  {
    "objectID": "project.html#reason-of-choice",
    "href": "project.html#reason-of-choice",
    "title": "Project Distribution of education levels by provinces and gender in Turkey over the years",
    "section": "2.3 Reason of Choice",
    "text": "2.3 Reason of Choice\nThe change in education level by province in Turkey over the years is an interesting data set. Comments can be made on how the relevant data has changed and developed over the years. For this reason, I wanted to examine in detail the changes in Turkey’s literacy rates and postgraduate education rates according to provinces and years.\nAdditionally, we will use a second data set to examine the relationship between education levels and crime rates. This data set provides us with crimes and crime rates according to education levels in 2020."
  },
  {
    "objectID": "project.html#preprocessing",
    "href": "project.html#preprocessing",
    "title": "Project Turky",
    "section": "2.4 Preprocessing",
    "text": "2.4 Preprocessing\nxxxxxx"
  },
  {
    "objectID": "project.html#exploratory-data-analysis",
    "href": "project.html#exploratory-data-analysis",
    "title": "Project Distribution of education levels by provinces and gender in Turkey over the years",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis\nBelow you see the graph of the total number of illiterate people in Turkey according to years. From the graph, it can be observed that the number of illiterate people in Turkey has decreased over the years. Stay tuned for many more summary data like this and analysis of the relationships between data.\n\n\nCode\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\neducation &lt;- read_excel(\"C:/Users/w10/Documents/GitHub/emu660-spring2024-ecemsenaunlu/data/education.xls\")\n\n\neducation2 &lt;- education %&gt;%\n  group_by(Year) %&gt;%\n  summarise(illiterateYearTotal = sum(illiterateTotal))\n\n\neducation2 %&gt;%\n  ggplot(aes(x = Year, y = illiterateYearTotal)) +\n  geom_bar(stat = \"identity\", fill = \"purple\") +\n  labs(title = \"Distribution of Illiterate Total Per Year\",\n       x = \"Year\",\n       y = \"Illiterate Total\") +\n theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n       axis.text.y = element_text(size = 8))\n\n\n\n\n\n\n\nCode\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(readxl)\nlibrary(ggplot2)\n\ncrime &lt;- read_excel(\"C:/Users/w10/Documents/GitHub/emu660-spring2024-ecemsenaunlu/data/crime.xls\")\n\ncrime &lt;- crime %&gt;% mutate(CRilliterateTotal = CRilliterateMale + CRilliterateFemale)\ncrime2 &lt;- crime %&gt;%\n  group_by(CRilliterateTotal)\n\n\ncrime2 %&gt;%\n  ggplot(aes(x = `Type of crime`, y = CRilliterateTotal)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  labs(title = \"Distribution of Illiterate Total Per Type Of Crime\",\n       x = \"Type of crime\",\n       y = \"Illiterate Total\") +\n theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n       axis.text.y = element_text(size = 8))"
  },
  {
    "objectID": "project.html#trend-analysis",
    "href": "project.html#trend-analysis",
    "title": "Project Distribution of education levels by provinces and gender in Turkey over the years",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis\nxxxxxx"
  },
  {
    "objectID": "project.html#model-fitting",
    "href": "project.html#model-fitting",
    "title": "Project Distribution of education levels by provinces and gender in Turkey over the years",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting\nxxxxxx"
  },
  {
    "objectID": "project.html#results",
    "href": "project.html#results",
    "title": "Project Distribution of education levels by provinces and gender in Turkey over the years",
    "section": "3.4 Results",
    "text": "3.4 Results\nxxxxxx"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignment I conducted for the [term and year, e.g. Spring 2024] EMU660 Decision Making with Analytics course.\nPlease use left menu to navigate through my assignments.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nFirm Turkish Aerospace, position Production Planning Engineer, year 2023 - ongoing.\nFirm Arlight Lighting, position Production Planning Engineer, 2022 to 2023."
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nFirm Metro Gross Market, position Graduation Project, year 2021\nFirm MitaE\u001f, position Candidate Engineer, year 2021"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/assignments/assignment-1.html",
    "href": "docs/assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nMy first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "My first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Assignment 2\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Tap and reach MY CV\nI am highly motivated to progress professionally, improve my professional knowledge, and acquire new ones."
  },
  {
    "objectID": "assignments/assignment-1.html#b",
    "href": "assignments/assignment-1.html#b",
    "title": "Assignment 1",
    "section": "",
    "text": "library(dslabs) data(“mtcars”)\nCreated a custom_summary function and which is works to calculating mean, standard variance, max value in data set, min value in data set and median value.\n\ncustom_summary &lt;- function (carCustom)\n{\n    meanValue &lt;- mean(carCustom)\n    std &lt;- sd(carCustom)\n    maxValue &lt;- max(carCustom)\n    minValue &lt;- min(carCustom)\n    medianValue &lt;- median(carCustom)\n    customStatistics &lt;- list(minValue,maxValue,meanValue,medianValue,std)\n    return (customStatistics)\n}\n\nThis stage custom_summary fuction calculated all values (mean, standard variance, max value in data set, min value in data set and median value) for each property of mtcars.\n\ncustomNames &lt;- names(mtcars)\nfor (customIndex in 1:11){\n    carCustom &lt;- mtcars[,customIndex]\n    customStatistics &lt;- custom_summary(carCustom)\n    print(customNames[customIndex])\n    print(customStatistics)\n}\n\n[1] \"mpg\"\n[[1]]\n[1] 10.4\n\n[[2]]\n[1] 33.9\n\n[[3]]\n[1] 20.09062\n\n[[4]]\n[1] 19.2\n\n[[5]]\n[1] 6.026948\n\n[1] \"cyl\"\n[[1]]\n[1] 4\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 6.1875\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 1.785922\n\n[1] \"disp\"\n[[1]]\n[1] 71.1\n\n[[2]]\n[1] 472\n\n[[3]]\n[1] 230.7219\n\n[[4]]\n[1] 196.3\n\n[[5]]\n[1] 123.9387\n\n[1] \"hp\"\n[[1]]\n[1] 52\n\n[[2]]\n[1] 335\n\n[[3]]\n[1] 146.6875\n\n[[4]]\n[1] 123\n\n[[5]]\n[1] 68.56287\n\n[1] \"drat\"\n[[1]]\n[1] 2.76\n\n[[2]]\n[1] 4.93\n\n[[3]]\n[1] 3.596563\n\n[[4]]\n[1] 3.695\n\n[[5]]\n[1] 0.5346787\n\n[1] \"wt\"\n[[1]]\n[1] 1.513\n\n[[2]]\n[1] 5.424\n\n[[3]]\n[1] 3.21725\n\n[[4]]\n[1] 3.325\n\n[[5]]\n[1] 0.9784574\n\n[1] \"qsec\"\n[[1]]\n[1] 14.5\n\n[[2]]\n[1] 22.9\n\n[[3]]\n[1] 17.84875\n\n[[4]]\n[1] 17.71\n\n[[5]]\n[1] 1.786943\n\n[1] \"vs\"\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 0.4375\n\n[[4]]\n[1] 0\n\n[[5]]\n[1] 0.5040161\n\n[1] \"am\"\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 0.40625\n\n[[4]]\n[1] 0\n\n[[5]]\n[1] 0.4989909\n\n[1] \"gear\"\n[[1]]\n[1] 3\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 3.6875\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 0.7378041\n\n[1] \"carb\"\n[[1]]\n[1] 1\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 2.8125\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 1.6152\n\n\nThe apply function and the custom_summary function were applied to all columns of the matrix.\n\nqq &lt;- apply(mtcars,2,custom_summary)\nprint(qq)\n\n$mpg\n$mpg[[1]]\n[1] 10.4\n\n$mpg[[2]]\n[1] 33.9\n\n$mpg[[3]]\n[1] 20.09062\n\n$mpg[[4]]\n[1] 19.2\n\n$mpg[[5]]\n[1] 6.026948\n\n\n$cyl\n$cyl[[1]]\n[1] 4\n\n$cyl[[2]]\n[1] 8\n\n$cyl[[3]]\n[1] 6.1875\n\n$cyl[[4]]\n[1] 6\n\n$cyl[[5]]\n[1] 1.785922\n\n\n$disp\n$disp[[1]]\n[1] 71.1\n\n$disp[[2]]\n[1] 472\n\n$disp[[3]]\n[1] 230.7219\n\n$disp[[4]]\n[1] 196.3\n\n$disp[[5]]\n[1] 123.9387\n\n\n$hp\n$hp[[1]]\n[1] 52\n\n$hp[[2]]\n[1] 335\n\n$hp[[3]]\n[1] 146.6875\n\n$hp[[4]]\n[1] 123\n\n$hp[[5]]\n[1] 68.56287\n\n\n$drat\n$drat[[1]]\n[1] 2.76\n\n$drat[[2]]\n[1] 4.93\n\n$drat[[3]]\n[1] 3.596563\n\n$drat[[4]]\n[1] 3.695\n\n$drat[[5]]\n[1] 0.5346787\n\n\n$wt\n$wt[[1]]\n[1] 1.513\n\n$wt[[2]]\n[1] 5.424\n\n$wt[[3]]\n[1] 3.21725\n\n$wt[[4]]\n[1] 3.325\n\n$wt[[5]]\n[1] 0.9784574\n\n\n$qsec\n$qsec[[1]]\n[1] 14.5\n\n$qsec[[2]]\n[1] 22.9\n\n$qsec[[3]]\n[1] 17.84875\n\n$qsec[[4]]\n[1] 17.71\n\n$qsec[[5]]\n[1] 1.786943\n\n\n$vs\n$vs[[1]]\n[1] 0\n\n$vs[[2]]\n[1] 1\n\n$vs[[3]]\n[1] 0.4375\n\n$vs[[4]]\n[1] 0\n\n$vs[[5]]\n[1] 0.5040161\n\n\n$am\n$am[[1]]\n[1] 0\n\n$am[[2]]\n[1] 1\n\n$am[[3]]\n[1] 0.40625\n\n$am[[4]]\n[1] 0\n\n$am[[5]]\n[1] 0.4989909\n\n\n$gear\n$gear[[1]]\n[1] 3\n\n$gear[[2]]\n[1] 5\n\n$gear[[3]]\n[1] 3.6875\n\n$gear[[4]]\n[1] 4\n\n$gear[[5]]\n[1] 0.7378041\n\n\n$carb\n$carb[[1]]\n[1] 1\n\n$carb[[2]]\n[1] 8\n\n$carb[[3]]\n[1] 2.8125\n\n$carb[[4]]\n[1] 2\n\n$carb[[5]]\n[1] 1.6152"
  },
  {
    "objectID": "assignments/assignment-1.html#c",
    "href": "assignments/assignment-1.html#c",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)\nThere is a Data Graph belongs to na_example data set.\n\nlibrary(dslabs)\ndata(\"na_example\")\nplot(na_example, type = \"l\", main = \"Data Graph\", xlab = \"Example Number\", ylab = \"Example Value\")\n\n\n\n\nYou can see all the values of the na_example data set in the table below.\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nNumber of elements which equal to NA in Data Set\n\nsum(is.na(na_example))\n\n[1] 145\n\n\nConvert NA’s to 660\n\nconvert_nas &lt;- ifelse(is.na(na_example),660, na_example) \n\nYou can see converted data below.\n\nprint(convert_nas)\n\n   [1]   2   1   3   2   1   3   1   4   3   2   2 660   2   2   1   4 660   1\n  [19]   1   2   1   2   2   1   2   5 660   2   2   3   1   2   4   1   1   1\n  [37]   4   5   2   3   4   1   2   4   1   1   2   1   5 660 660 660   1   1\n  [55]   5   1   3   1 660   4   4   7   3   2 660 660   1 660   4   1   2   2\n  [73]   3   2   1   2   2   4   3   4   2   3   1   3   2   1   1   1   3   1\n  [91] 660   3   1   2   2   1   2   2   1   1   4   1   1   2   3   3   2   2\n [109]   3   3   3   4   1   1   1   2 660   4   3   4   3   1   2   1 660 660\n [127] 660 660   1   5   1   2   1   3   5   3   2   2 660 660 660 660   3   5\n [145]   3   1   1   4   2   4   3   3 660   2   3   2   6 660   1   1   2   2\n [163]   1   3   1   1   5 660 660   2   4 660   2   5   1   4   3   3 660   4\n [181]   3   1   4   1   1   3   1   1 660 660   3   5   2   2   2   3   1   2\n [199]   2   3   2   1 660   2 660   1 660 660   2   1   1 660   3 660   1   2\n [217]   2   1   3   2   2   1   1   2   3   1   1   1   4   3   4   2   2   1\n [235]   4   1 660   5   1   4 660   3 660 660   1   1   5   2   3   3   2   4\n [253] 660   3   2   5 660   2   3   4   6   2   2   2 660   2 660   2 660   3\n [271]   3   2   2   4   3   1   4   2 660   2   4 660   6   2   3   1 660   2\n [289]   2 660   1   1   3   2   3   3   1 660   1   4   2   1   1   3   2   1\n [307]   2   3   1 660   2   3   3   2   1   2   3   5   5   1   2   3   3   1\n [325] 660 660   1   2   4 660   2   1   1   1   3   2   1   1   3   4 660   1\n [343]   2   1   1   3   3 660   1   1   3   5   3   2   3   4   1   4   3   1\n [361] 660   2   1   2   2   1   2   2   6   1   2   4   5 660   3   4   2   1\n [379]   1   4   2   1   1   1   1   2   1   4   4   1   3 660   3   3 660   2\n [397] 660   1   2   1   1   4   2   1   4   4 660   1   2 660   3   2   2   2\n [415]   1   4   3   6   1   2   3   1   3   2   2   2   1   1   3   2   1   1\n [433]   1   3   2   2 660   4   4   4   1   1 660   4   3 660   1   3   1   3\n [451]   2   4   2   2   2   3   2   1   4   3 660   1   4   3   1   3   2 660\n [469]   3 660   1   3   1   4   1   1   1   2   4   3   1   2   2   2   3   2\n [487]   3   1   1 660   3   2   1   1   2 660   2   2   2   3   3   1   1   2\n [505] 660   1   2   1   1   3   3   1   3   1   1   1   1   1   2   5   1   1\n [523]   2   2   1   1 660   1   4   1   2   4   1   3   2 660   1   1 660   2\n [541]   1   1   4   2   3   3   1   5   3   1   1   2 660   1   1   3   1   3\n [559]   2   4 660   2   3   2   1   2   1   1   1   2   2   3   1   5   2 660\n [577]   2 660   3   2   2   2   1   5   3   2   3   1 660   3   1   2   2   2\n [595]   1   2   2   4 660   6   1   2 660   1   1   2   2   3 660   3   2   3\n [613]   3   4   2 660   2 660   4 660   1   1   2   2   3   1   1   1   3 660\n [631]   2   5 660   7   1 660   4   3   3   1 660   1   1   1   1   3   2   4\n [649]   2   2   3 660 660   1   4   3   2   2   2   3   2   4   2   2   4 660\n [667] 660 660   6   3   3   1   4   4   2   1 660   1   6 660   3   3   2   1\n [685]   1   6 660   1   5   1 660   2   6   2 660   4   1   3   1   2 660   1\n [703]   1   3   1   2   4   2   1   3   2   4   3   2   2   1   1   5   6   4\n [721]   2   2   2   2   4 660   1   2   2   2   2   4   5 660 660 660   4   3\n [739]   3   3   2   4   2   4 660 660 660 660   2   1 660   2   4   3   2 660\n [757]   2   3   1   3   4 660   1   2   1   2 660   3   1   2   1   2   1   2\n [775]   1   2   2   2   2   1   1   3   3   1   3   4   3 660 660   4   2   3\n [793]   2   1   3   2   4   2   2   3   1   2   4   3   3   4 660   1   4   2\n [811]   1   1   1   3   1   5   2   2   4   2 660   1   3   1   2 660   1   2\n [829]   1   2   1 660   1   3   2   3   2 660   2   1   4   2 660 660 660   2\n [847]   4   2 660 660   3   1 660   5   5   2   2   2 660   2   1   3   1   3\n [865]   2   4   2   4 660   4   1   2   3   2   3   3   2   3   2   2   2   1\n [883]   3   2   4   2 660   3   3   2   2 660 660   3   2   1   2   4   1   1\n [901]   1   1   4   3   2 660   3   2 660   1 660   3   2   1   1   1   2 660\n [919]   2   2   3   3   2 660 660   4   5   2   2   2   1   2   3   1   3   3\n [937]   4   3 660   1   1   1 660   4   3   5   1   1   2 660   2   2   2   2\n [955]   5   2   2   3   1   2   3 660   1   2 660 660   2 660   3   1   1   2\n [973]   5   3   5   1   1   4 660   2   1   3   1   1   2   4   3   3   3 660\n [991]   1   1   2   2   1   1   2   2 660   2\n\n\nTotal NA values at updated data set.\n\nsum(is.na(convert_nas))\n\n[1] 0\n\n\nNumber of NA elements which are assigned to 660.\n\nsumpro&lt;-sum(convert_nas==660)\nprint(sumpro)\n\n[1] 145"
  },
  {
    "objectID": "assignments/assignment-1.html#a-conversations-on-data-science-and-industrial-engineering---mustafa-baydoan-erdi-dademir",
    "href": "assignments/assignment-1.html#a-conversations-on-data-science-and-industrial-engineering---mustafa-baydoan-erdi-dademir",
    "title": "Assignment 1",
    "section": "",
    "text": "During an interview on Data Science and Industrial Engineering, Mustafa Baydo??an shared valuable insights on how to solve real-life problems with data science. To start, let me briefly introduce Mustafa Baydo??an. He is an instructor at Bo??azi??i University’s Industrial Engineering department and also the founder of Algopoly, a software and consultancy company located in Istanbul. His research focuses on using data science tools and techniques for large-scale data mining, time series analysis, pattern discovery, and operations research. He received his undergraduate and graduate degrees from METU and his doctorate from Arizona State University. In his Master’s degree, he worked on multi-criteria decision-making and later shifted his focus to data science in his doctoral studies, where he worked on topics such as time series analysis, forecasting and creating solutions with machine learning methods. After gaining experience in consultancy and private sector, he founded Algopoly in 2017, where the company primarily serves the energy and logistics sectors.\nDuring the interview, Mustafa Baydo??an discussed several real-life problems that data science. For instance, he mentioned a several company demand forecast study is being carry out for shoe sales. He mentions that forecasting applications should be used to solve this problem. He also shared an example of a company in the USA that owns forests used for timber production. After the drying process, some timbers become warped. It is essential to estimate how much of these timbers will be bent as the value of timber in the market varies depending on its shape. By using image processing and machine learning methods, data can be collected by merging the images and processing the information to create a dataset. With this solution, the company can identify the timbers that are likely to become bent before the drying process, and corrective actions can be taken at a low cost. According to Mustafa Baydo??an, this can increase profits by 5 percent.\nFurthermore, he also talked about energy consumption and production balance. Forecasting is frequently used to ensure balance in electricity markets and establish the balance between production and consumption. If the forecast does not come true, an imbalance occurs, and companies incur costs. To prevent this, estimates can be made by calculating the production amount based on regional consumption data and taking into account special situations.\nIn addition, Mustafa Baydo??an shared an example of Trendyol, another data processing company that uses models to make predictions based on the number of units a product will sell and whether anyone has looked at the product before. In Trendyol’s model, penalty coefficients are assigned to products that are frequently viewed but not purchased, thus preventing them from appearing in front of the user. By doing this, Trendyol can provide its users with a better shopping experience by eliminating irrelevant options.\nOverall, Mustafa Baydo??an’s insights were valuable and enlightening, providing a glimpse into how data science can be used to solve real-life problems from different perspectives."
  },
  {
    "objectID": "assignments/assignment-1.html#a-conversations-on-data-science-and-industrial-engineering---mustafa-baydogan-erdi-dasdemir",
    "href": "assignments/assignment-1.html#a-conversations-on-data-science-and-industrial-engineering---mustafa-baydogan-erdi-dasdemir",
    "title": "Assignment 1",
    "section": "(a) Conversations on Data Science and Industrial Engineering - Mustafa Baydogan, Erdi Dasdemir",
    "text": "(a) Conversations on Data Science and Industrial Engineering - Mustafa Baydogan, Erdi Dasdemir\nDuring an interview on Data Science and Industrial Engineering, Mustafa Baydogan shared valuable insights on how to solve real-life problems with data science. To start, let me briefly introduce Mustafa Baydogan. He is an instructor at Bogazici University’s Industrial Engineering department and also the founder of Algopoly, a software and consultancy company located in Istanbul. His research focuses on using data science tools and techniques for large-scale data mining, time series analysis, pattern discovery, and operations research. He received his undergraduate and graduate degrees from METU and his doctorate from Arizona State University. In his Master’s degree, he worked on multi-criteria decision-making and later shifted his focus to data science in his doctoral studies, where he worked on topics such as time series analysis, forecasting and creating solutions with machine learning methods. After gaining experience in consultancy and private sector, he founded Algopoly in 2017, where the company primarily serves the energy and logistics sectors.\nDuring the interview, Mustafa Baydogan discussed several real-life problems that data science. For instance, he mentioned a several company demand forecast study is being carry out for shoe sales. He mentions that forecasting applications should be used to solve this problem. He also shared an example of a company in the USA that owns forests used for timber production. After the drying process, some timbers become warped. It is essential to estimate how much of these timbers will be bent as the value of timber in the market varies depending on its shape. By using image processing and machine learning methods, data can be collected by merging the images and processing the information to create a dataset. With this solution, the company can identify the timbers that are likely to become bent before the drying process, and corrective actions can be taken at a low cost. According to Mustafa Baydogan, this can increase profits by 5 percent.\nFurthermore, he also talked about energy consumption and production balance. Forecasting is frequently used to ensure balance in electricity markets and establish the balance between production and consumption. If the forecast does not come true, an imbalance occurs, and companies incur costs. To prevent this, estimates can be made by calculating the production amount based on regional consumption data and taking into account special situations.\nIn addition, Mustafa Baydogan shared an example of Trendyol, another data processing company that uses models to make predictions based on the number of units a product will sell and whether anyone has looked at the product before. In Trendyols model, penalty coefficients are assigned to products that are frequently viewed but not purchased, thus preventing them from appearing in front of the user. By doing this, Trendyol can provide its users with a better shopping experience by eliminating irrelevant options.\nOverall, Mustafa Baydogan’s insights were valuable and enlightening, providing a glimpse into how data science can be used to solve real-life problems from different perspectives."
  },
  {
    "objectID": "assignments/assignment-1.html#b-created-a-custom_summary-function-and-which-is-works-to-calculating-mean-standard-variance-max-value-in-data-set-min-value-in-data-set-and-median-value.",
    "href": "assignments/assignment-1.html#b-created-a-custom_summary-function-and-which-is-works-to-calculating-mean-standard-variance-max-value-in-data-set-min-value-in-data-set-and-median-value.",
    "title": "Assignment 1",
    "section": "(b) Created a custom_summary function and which is works to calculating mean, standard variance, max value in data set, min value in data set and median value.",
    "text": "(b) Created a custom_summary function and which is works to calculating mean, standard variance, max value in data set, min value in data set and median value.\n\ncustom_summary &lt;- function (carCustom)\n{\n    meanValue &lt;- mean(carCustom)\n    std &lt;- sd(carCustom)\n    maxValue &lt;- max(carCustom)\n    minValue &lt;- min(carCustom)\n    medianValue &lt;- median(carCustom)\n    customStatistics &lt;- list(minValue,maxValue,meanValue,medianValue,std)\n    return (customStatistics)\n}\n\nThis stage custom_summary fuction calculated all values (mean, standard variance, max value in data set, min value in data set and median value) for each property of mtcars.\n\ncustomNames &lt;- names(mtcars)\nfor (customIndex in 1:11){\n    carCustom &lt;- mtcars[,customIndex]\n    customStatistics &lt;- custom_summary(carCustom)\n    print(customNames[customIndex])\n    print(customStatistics)\n}\n\n[1] \"mpg\"\n[[1]]\n[1] 10.4\n\n[[2]]\n[1] 33.9\n\n[[3]]\n[1] 20.09062\n\n[[4]]\n[1] 19.2\n\n[[5]]\n[1] 6.026948\n\n[1] \"cyl\"\n[[1]]\n[1] 4\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 6.1875\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 1.785922\n\n[1] \"disp\"\n[[1]]\n[1] 71.1\n\n[[2]]\n[1] 472\n\n[[3]]\n[1] 230.7219\n\n[[4]]\n[1] 196.3\n\n[[5]]\n[1] 123.9387\n\n[1] \"hp\"\n[[1]]\n[1] 52\n\n[[2]]\n[1] 335\n\n[[3]]\n[1] 146.6875\n\n[[4]]\n[1] 123\n\n[[5]]\n[1] 68.56287\n\n[1] \"drat\"\n[[1]]\n[1] 2.76\n\n[[2]]\n[1] 4.93\n\n[[3]]\n[1] 3.596563\n\n[[4]]\n[1] 3.695\n\n[[5]]\n[1] 0.5346787\n\n[1] \"wt\"\n[[1]]\n[1] 1.513\n\n[[2]]\n[1] 5.424\n\n[[3]]\n[1] 3.21725\n\n[[4]]\n[1] 3.325\n\n[[5]]\n[1] 0.9784574\n\n[1] \"qsec\"\n[[1]]\n[1] 14.5\n\n[[2]]\n[1] 22.9\n\n[[3]]\n[1] 17.84875\n\n[[4]]\n[1] 17.71\n\n[[5]]\n[1] 1.786943\n\n[1] \"vs\"\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 0.4375\n\n[[4]]\n[1] 0\n\n[[5]]\n[1] 0.5040161\n\n[1] \"am\"\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 0.40625\n\n[[4]]\n[1] 0\n\n[[5]]\n[1] 0.4989909\n\n[1] \"gear\"\n[[1]]\n[1] 3\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 3.6875\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 0.7378041\n\n[1] \"carb\"\n[[1]]\n[1] 1\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 2.8125\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 1.6152\n\n\nThe apply function and the custom_summary function were applied to all columns of the matrix.\n\nqq &lt;- apply(mtcars,2,custom_summary)\nprint(qq)\n\n$mpg\n$mpg[[1]]\n[1] 10.4\n\n$mpg[[2]]\n[1] 33.9\n\n$mpg[[3]]\n[1] 20.09062\n\n$mpg[[4]]\n[1] 19.2\n\n$mpg[[5]]\n[1] 6.026948\n\n\n$cyl\n$cyl[[1]]\n[1] 4\n\n$cyl[[2]]\n[1] 8\n\n$cyl[[3]]\n[1] 6.1875\n\n$cyl[[4]]\n[1] 6\n\n$cyl[[5]]\n[1] 1.785922\n\n\n$disp\n$disp[[1]]\n[1] 71.1\n\n$disp[[2]]\n[1] 472\n\n$disp[[3]]\n[1] 230.7219\n\n$disp[[4]]\n[1] 196.3\n\n$disp[[5]]\n[1] 123.9387\n\n\n$hp\n$hp[[1]]\n[1] 52\n\n$hp[[2]]\n[1] 335\n\n$hp[[3]]\n[1] 146.6875\n\n$hp[[4]]\n[1] 123\n\n$hp[[5]]\n[1] 68.56287\n\n\n$drat\n$drat[[1]]\n[1] 2.76\n\n$drat[[2]]\n[1] 4.93\n\n$drat[[3]]\n[1] 3.596563\n\n$drat[[4]]\n[1] 3.695\n\n$drat[[5]]\n[1] 0.5346787\n\n\n$wt\n$wt[[1]]\n[1] 1.513\n\n$wt[[2]]\n[1] 5.424\n\n$wt[[3]]\n[1] 3.21725\n\n$wt[[4]]\n[1] 3.325\n\n$wt[[5]]\n[1] 0.9784574\n\n\n$qsec\n$qsec[[1]]\n[1] 14.5\n\n$qsec[[2]]\n[1] 22.9\n\n$qsec[[3]]\n[1] 17.84875\n\n$qsec[[4]]\n[1] 17.71\n\n$qsec[[5]]\n[1] 1.786943\n\n\n$vs\n$vs[[1]]\n[1] 0\n\n$vs[[2]]\n[1] 1\n\n$vs[[3]]\n[1] 0.4375\n\n$vs[[4]]\n[1] 0\n\n$vs[[5]]\n[1] 0.5040161\n\n\n$am\n$am[[1]]\n[1] 0\n\n$am[[2]]\n[1] 1\n\n$am[[3]]\n[1] 0.40625\n\n$am[[4]]\n[1] 0\n\n$am[[5]]\n[1] 0.4989909\n\n\n$gear\n$gear[[1]]\n[1] 3\n\n$gear[[2]]\n[1] 5\n\n$gear[[3]]\n[1] 3.6875\n\n$gear[[4]]\n[1] 4\n\n$gear[[5]]\n[1] 0.7378041\n\n\n$carb\n$carb[[1]]\n[1] 1\n\n$carb[[2]]\n[1] 8\n\n$carb[[3]]\n[1] 2.8125\n\n$carb[[4]]\n[1] 2\n\n$carb[[5]]\n[1] 1.6152"
  },
  {
    "objectID": "assignments/assignment-1.html#c-na-example",
    "href": "assignments/assignment-1.html#c-na-example",
    "title": "Assignment 1",
    "section": "(c) NA Example",
    "text": "(c) NA Example\nThere is a Data Graph belongs to na_example data set.\n\nlibrary(dslabs)\ndata(\"na_example\")\nplot(na_example, type = \"l\", main = \"Data Graph\", xlab = \"Example Number\", ylab = \"Example Value\")\n\n\n\n\nYou can see all the values of the na_example data set in the table below.\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nNumber of elements which equal to NA in Data Set\n\nsum(is.na(na_example))\n\n[1] 145\n\n\nConvert NA’s to 660\n\nconvert_nas &lt;- ifelse(is.na(na_example),660, na_example) \n\nYou can see converted data below.\n\nprint(convert_nas)\n\n   [1]   2   1   3   2   1   3   1   4   3   2   2 660   2   2   1   4 660   1\n  [19]   1   2   1   2   2   1   2   5 660   2   2   3   1   2   4   1   1   1\n  [37]   4   5   2   3   4   1   2   4   1   1   2   1   5 660 660 660   1   1\n  [55]   5   1   3   1 660   4   4   7   3   2 660 660   1 660   4   1   2   2\n  [73]   3   2   1   2   2   4   3   4   2   3   1   3   2   1   1   1   3   1\n  [91] 660   3   1   2   2   1   2   2   1   1   4   1   1   2   3   3   2   2\n [109]   3   3   3   4   1   1   1   2 660   4   3   4   3   1   2   1 660 660\n [127] 660 660   1   5   1   2   1   3   5   3   2   2 660 660 660 660   3   5\n [145]   3   1   1   4   2   4   3   3 660   2   3   2   6 660   1   1   2   2\n [163]   1   3   1   1   5 660 660   2   4 660   2   5   1   4   3   3 660   4\n [181]   3   1   4   1   1   3   1   1 660 660   3   5   2   2   2   3   1   2\n [199]   2   3   2   1 660   2 660   1 660 660   2   1   1 660   3 660   1   2\n [217]   2   1   3   2   2   1   1   2   3   1   1   1   4   3   4   2   2   1\n [235]   4   1 660   5   1   4 660   3 660 660   1   1   5   2   3   3   2   4\n [253] 660   3   2   5 660   2   3   4   6   2   2   2 660   2 660   2 660   3\n [271]   3   2   2   4   3   1   4   2 660   2   4 660   6   2   3   1 660   2\n [289]   2 660   1   1   3   2   3   3   1 660   1   4   2   1   1   3   2   1\n [307]   2   3   1 660   2   3   3   2   1   2   3   5   5   1   2   3   3   1\n [325] 660 660   1   2   4 660   2   1   1   1   3   2   1   1   3   4 660   1\n [343]   2   1   1   3   3 660   1   1   3   5   3   2   3   4   1   4   3   1\n [361] 660   2   1   2   2   1   2   2   6   1   2   4   5 660   3   4   2   1\n [379]   1   4   2   1   1   1   1   2   1   4   4   1   3 660   3   3 660   2\n [397] 660   1   2   1   1   4   2   1   4   4 660   1   2 660   3   2   2   2\n [415]   1   4   3   6   1   2   3   1   3   2   2   2   1   1   3   2   1   1\n [433]   1   3   2   2 660   4   4   4   1   1 660   4   3 660   1   3   1   3\n [451]   2   4   2   2   2   3   2   1   4   3 660   1   4   3   1   3   2 660\n [469]   3 660   1   3   1   4   1   1   1   2   4   3   1   2   2   2   3   2\n [487]   3   1   1 660   3   2   1   1   2 660   2   2   2   3   3   1   1   2\n [505] 660   1   2   1   1   3   3   1   3   1   1   1   1   1   2   5   1   1\n [523]   2   2   1   1 660   1   4   1   2   4   1   3   2 660   1   1 660   2\n [541]   1   1   4   2   3   3   1   5   3   1   1   2 660   1   1   3   1   3\n [559]   2   4 660   2   3   2   1   2   1   1   1   2   2   3   1   5   2 660\n [577]   2 660   3   2   2   2   1   5   3   2   3   1 660   3   1   2   2   2\n [595]   1   2   2   4 660   6   1   2 660   1   1   2   2   3 660   3   2   3\n [613]   3   4   2 660   2 660   4 660   1   1   2   2   3   1   1   1   3 660\n [631]   2   5 660   7   1 660   4   3   3   1 660   1   1   1   1   3   2   4\n [649]   2   2   3 660 660   1   4   3   2   2   2   3   2   4   2   2   4 660\n [667] 660 660   6   3   3   1   4   4   2   1 660   1   6 660   3   3   2   1\n [685]   1   6 660   1   5   1 660   2   6   2 660   4   1   3   1   2 660   1\n [703]   1   3   1   2   4   2   1   3   2   4   3   2   2   1   1   5   6   4\n [721]   2   2   2   2   4 660   1   2   2   2   2   4   5 660 660 660   4   3\n [739]   3   3   2   4   2   4 660 660 660 660   2   1 660   2   4   3   2 660\n [757]   2   3   1   3   4 660   1   2   1   2 660   3   1   2   1   2   1   2\n [775]   1   2   2   2   2   1   1   3   3   1   3   4   3 660 660   4   2   3\n [793]   2   1   3   2   4   2   2   3   1   2   4   3   3   4 660   1   4   2\n [811]   1   1   1   3   1   5   2   2   4   2 660   1   3   1   2 660   1   2\n [829]   1   2   1 660   1   3   2   3   2 660   2   1   4   2 660 660 660   2\n [847]   4   2 660 660   3   1 660   5   5   2   2   2 660   2   1   3   1   3\n [865]   2   4   2   4 660   4   1   2   3   2   3   3   2   3   2   2   2   1\n [883]   3   2   4   2 660   3   3   2   2 660 660   3   2   1   2   4   1   1\n [901]   1   1   4   3   2 660   3   2 660   1 660   3   2   1   1   1   2 660\n [919]   2   2   3   3   2 660 660   4   5   2   2   2   1   2   3   1   3   3\n [937]   4   3 660   1   1   1 660   4   3   5   1   1   2 660   2   2   2   2\n [955]   5   2   2   3   1   2   3 660   1   2 660 660   2 660   3   1   1   2\n [973]   5   3   5   1   1   4 660   2   1   3   1   1   2   4   3   3   3 660\n [991]   1   1   2   2   1   1   2   2 660   2\n\n\nTotal NA values at updated data set.\n\nsum(is.na(convert_nas))\n\n[1] 0\n\n\nNumber of NA elements which are assigned to 660.\n\nsumpro&lt;-sum(convert_nas==660)\nprint(sumpro)\n\n[1] 145"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! My name is Ecem Sena Unlu\nThis is my personal webpage..\nPlease stay tuned to follow my works on data analytics, blog posts, and more."
  }
]