[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! My name is Ecem Sena Unlu\nThis is my personal webpage..\nPlease stay tuned to follow my works on data analytics, blog posts, and more."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project Turkey’s Education Istatistics",
    "section": "",
    "text": "Welcome to my project page.\nKeep an eye on this space to stay updated with my project activities."
  },
  {
    "objectID": "project.html#data-source",
    "href": "project.html#data-source",
    "title": "Project Turkey’s Education Istatistics",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\nCompleted education by province (15+ years old)\n2020 Data set of Convicts Entered into Penal Institutions by Crime Type and Educational Level, crime situations in the province, Table 5\nAddress-Based Population Registration System Results, 2020, Table 3\nGross Domestic Product (GDP) 2018,2019,2020, Table 7\n2020 Data set of Convicts Entered into Penal Institutions by Crime Type and Educational Level, crime situations in the province, Table 4\nYou can access following data sets from the link below:\nReach Out To Data Source"
  },
  {
    "objectID": "project.html#general-information-about-data",
    "href": "project.html#general-information-about-data",
    "title": "Project Turkey’s Education Istatistics",
    "section": "2.2 General Information About Data",
    "text": "2.2 General Information About Data\nBelow you can find the names of all columns in the Education and Crime datasets and information about the data they represent.\nYear : The Year where data is collected for statistical calculations.\nProvinceID : The ProvinceID where data is collected for statistical calculations.\nProvinceName : The ProvinceName where data is collected for statistical calculations.\nilliterateTotal : The illiterateTotal where Total number of illiterate people in Turkey.\nilliterateMale : The IlliterateMale where Total number of illiterate men in Turkey.\nilliterateFemale : The IlliterateFemale where Total number of illiterate women in Turkey.\nLiterateWithoutADiplomaTotal :The LiterateWithoutADiplomaTotal is total number of people in Turkey which has no diploma.\nLiterateWithoutADiplomaMale :The LiterateWithoutADiplomaTotal is total number of men in Turkey which has no diploma.\nLiterateWithoutADiplomaFemale :The LiterateWithoutADiplomaTotal is total number of women in Turkey which has no diploma.\nPrimarySchoolTotal : The PrimarySchoolTotal is number of people which graduated from Primary school in Turkey.\nPrimarySchoolMale : The PrimarySchoolMale is number of men which graduated from Primary school in Turkey.\nPrimarySchoolFemale : The PrimarySchoolMale is number of women which graduated from Primary school in Turkey.\nPrimaryEducationTotal : The PrimaryEducationTotal is number of people which graduated from Primary Education in Turkey. \nPrimaryEducationMale : The PrimaryEducationMale is number of men which graduated from Primary Education in Turkey.\nPrimaryEducationFemale : The PrimaryEducationFemale is number of women which graduated from Primary Education in Turkey.\nLowerSecondarySchoolTotal : The LowerSecondarySchoolTotal is number of people which graduated from Lower Secondary School in Turkey.\nLowerSecondarySchoolMale :The LowerSecondarySchoolMale is number of men which graduated from Lower Secondary School in Turkey.\nLowerSecondarySchoolFemale :The LowerSecondarySchoolFemale is number of women which graduated from Lower Secondary School in Turkey.\nUpperSecondarySchoolTotal :The UpperSecondarySchoolTotal is number of people which graduated from Upper Secondary School in Turkey.\nUpperSecondarySchoolMale :The LowerSecondarySchoolMale is number of men which graduated from Upper Secondary School in Turkey.\nUpperSecondarySchoolFemale :The LowerSecondarySchoolFemale is number of women which graduated from Upper Secondary School in Turkey.\nHigherEducationalinstitutionsTotal :The HigherEducationalinstitutionsTotal is number of people which graduated from Higher Educational institutions in Turkey.\nHigherEducationalinstitutionsMale :The HigherEducationalinstitutionsMale is number of men which graduated from Higher Educational institutions in Turkey.\nHigherEducationalinstitutionsFemale :The HigherEducationalinstitutionsFemale is number of women which graduated from Higher Educational institutions in Turkey.\nMasterTotal :The MasterTotal is number of people which graduated from Master in Turkey.\nMasterMale : The MasterTotal is number of men which graduated from Master in Turkey.\nMasterFemale : The MasterTotal is number of women which graduated from Master in Turkey.\nDoctorateTotal : The DoctorateTotal is number of people which graduated from Master in Turkey.\nDoctorateMale :The DoctorateTotal is number of men which graduated from Master in Turkey.\nDoctorateFemale :The DoctorateTotal is number of women which graduated from Master in Turkey.\nUnknownTotal : The UnknownTotal is the number of people whose educational status is unknown in Turkey.\nUnknownMale :The UnknownTotal is the number of men whose educational status is unknown in Turkey.\nUnknownFemale :The UnknownTotal is the number of women whose educational status is unknown in Turkey.\nBelow you can find the names of all columns in the Population dataset and information about the data they represent.\nProvinces : The Provinces where data is collected for statistical calculations.\nTotalPopulation : TotalPopulation data gives the total number of people living in that city.\nTotalFemale : TotalPopulation data gives the total number of women living in that city.\nTotalMale : TotalPopulation data gives the total number of men living in that city.\nBelow you can find the names of all columns in the GSYMH dataset and information about the data they represent.\nProvinceNames : The Provinces where data is collected for statistical calculations.\n2018 : It is the contribution rate of cities to Turkey’s Gross National Product in 2018.\n2019 : It is the contribution rate of cities to Turkey’s Gross National Product in 2019.\n2020 : It is the contribution rate of cities to Turkey’s Gross National Product in 2020.\nBelow you can find the names of all columns in the GDP dataset and information about the data they represent.\nProvinceNames : The Provinces where data is collected for statistical calculations.\n2018 : It is the Gross National Product per capita in cities in Türkiye in 2018.\n2019 : It is the Gross National Product per capita in cities in Türkiye in 2019.\n2020 : It is the Gross National Product per capita in cities in Türkiye in 2020."
  },
  {
    "objectID": "project.html#reason-of-choice",
    "href": "project.html#reason-of-choice",
    "title": "Project Turkey’s Education Istatistics",
    "section": "2.3 Reason of Choice",
    "text": "2.3 Reason of Choice\nThe change in education level by province in Turkey over the years is an interesting data set. Comments can be made on how the relevant data has changed and developed over the years. For this reason, the changes in Turkey’s literacy rates and postgraduate education rates according to provinces and years will be examined in detail.\nAdditionally, a second data set will be used to examine the relationship between education levels and crime rates. This data set provides us with crimes and crime rates according to education levels in 2020. In addition, it is seen that the 2020 population numbers of the provinces are an important data source for the correct evaluation of the relevant data.\nIn the yearCrime data set, data on crime rates according to years and education levels is presented. It is aimed to perform time series analysis using relevant data.\nIn addition, another data set showing the contribution of provinces to Turkey’s gross national product will also be used. I estimate that there is a linear relationship between these data and graduate education rates in cities. A regression analysis will be conducted in this direction."
  },
  {
    "objectID": "project.html#preprocessing",
    "href": "project.html#preprocessing",
    "title": "Project Turky",
    "section": "2.4 Preprocessing",
    "text": "2.4 Preprocessing\nxxxxxx"
  },
  {
    "objectID": "project.html#exploratory-data-analysis",
    "href": "project.html#exploratory-data-analysis",
    "title": "Project Turkey’s Education Istatistics",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis\nBelow you see the graph of the total number of illiterate people in Turkey according to years. From the graph, it can be observed that the number of illiterate people in Turkey has decreased over the years. Stay tuned for many more summary data like this and analysis of the relationships between data.\n\n\nCode\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\neducation &lt;- read_excel(\"C:/Users/w10/Documents/GitHub/emu660-spring2024-ecemsenaunlu/data/education.xls\")\n\n\neducation2 &lt;- education %&gt;%\n  group_by(Year) %&gt;%\n  summarise(illiterateYearTotal = sum(illiterateTotal))\n\n\neducation2 %&gt;%\n  ggplot(aes(x = Year, y = illiterateYearTotal)) +\n  geom_bar(stat = \"identity\", fill = \"purple\") +\n  labs(title = \"Distribution of Illiterate Total Per Year\",\n       x = \"Year\",\n       y = \"Illiterate Total\") +\n theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n       axis.text.y = element_text(size = 8))\n\n\n\n\n\n\n\nCode\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(readxl)\nlibrary(ggplot2)\n\ncrime &lt;- read_excel(\"C:/Users/w10/Documents/GitHub/emu660-spring2024-ecemsenaunlu/data/crime.xls\")\n\ncrime &lt;- crime %&gt;% mutate(CRilliterateTotal = CRilliterateMale + CRilliterateFemale)\ncrime2 &lt;- crime %&gt;%\n  group_by(CRilliterateTotal)\n\n\ncrime2 %&gt;%\n  ggplot(aes(x = `Type of crime`, y = CRilliterateTotal)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  labs(title = \"Distribution of Illiterate Total Per Type Of Crime\",\n       x = \"Type of crime\",\n       y = \"Illiterate Total\") +\n theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n       axis.text.y = element_text(size = 8))"
  },
  {
    "objectID": "project.html#trend-analysis",
    "href": "project.html#trend-analysis",
    "title": "Project Turkey’s Education Istatistics",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis\nFirst of all, how many doctoral and master’s students were in each city in 2020, these statistics will be discussed.\n\n\nCode\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\neducation &lt;- read_excel(\"C:/Users/w10/Documents/GitHub/emu660-spring2024-ecemsenaunlu/data/education.xls\")\n\n\neducation2020 &lt;- education %&gt;% filter(Year == 2020)\n\neducation2020 %&gt;%\n  ggplot(aes(x = ProvinceName, y = DoctorateTotal)) +\n  geom_bar(stat = \"identity\", fill = \"purple\") +\n  labs(title = \"Distribution of Doctorate Total per Province in 2020\",\n       x = \"Province\",\n       y = \"Doctorate Total\") +\n theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n       axis.text.y = element_text(size = 8))"
  },
  {
    "objectID": "project.html#model-fitting",
    "href": "project.html#model-fitting",
    "title": "Project Turkey’s Education Istatistics",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting\nFirst of all, in Exploratory Data Analysis, it is seen that the training data alone is insufficient to explain the data. For this reason, it was decided to benefit from the population data of 2020. The data on the proportion of people with doctorate and master’s degrees in which city are located compared to the population will be examined.\n\n\nCode\nlibrary(dplyr)\nresult &lt;- inner_join(education2020, population, by = \"ProvinceName\")\n\nDoctorateResult &lt;- result %&gt;%\n  mutate(DoctoratePercentage = DoctorateTotal / TotalPopulation * 100 )\n\nDoctorate &lt;- DoctorateResult %&gt;%\n  ggplot() +\n  geom_bar(aes(x = ProvinceName, y = DoctoratePercentage), \n           stat = \"identity\", fill = \"blue\", position = \"dodge\") + \n  labs(title = \"Distribution of Doctorate Total per Total Population by Province in 2020\",\n       x = \"Province\",\n       y = \"Percentage\") +\n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n        axis.text.y = element_text(size = 8))\n\n\nresult &lt;- inner_join(education2020, population, by = \"ProvinceName\")\n\nMasterResult &lt;- result %&gt;%\n  mutate(MasterPercentage = MasterTotal / TotalPopulation * 100)\n\nMaster &lt;- MasterResult %&gt;%\n  ggplot() +\n  geom_bar(aes(x = ProvinceName, y = MasterPercentage), \n           stat = \"identity\", fill = \"purple\", position = \"dodge\") + \n  labs(title = \"Distribution of Master Total per Total Population by Province in 2020\",\n       x = \"Province\",\n       y = \"Percentage\") +\n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n        axis.text.y = element_text(size = 8))\n\nlibrary(gridExtra)\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\ngrid.arrange(Doctorate, Master, nrow = 2)\n\n\n\n\n\nWe see that Ankara is the province with the highest percentage of people with doctorate and master’s degrees in 2020. It seems that the cities in the top 10 list with a high percentage of both master’s and doctorate degrees are Ankara, Isparta, Eskişehir, Edirne, Canakkale, Izmir and Istanbul. At the same time, we aim to filter the provinces with the highest number of illiterate people and see if they can be associated with the general structure of the provinces.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\ntop_master &lt;- MasterResult %&gt;%\n  arrange(desc(MasterPercentage)) %&gt;%\n  head(10)\n\ntop_doctorate &lt;- DoctorateResult %&gt;%\n  arrange(desc(DoctoratePercentage)) %&gt;%\n  head(10)\n\ntopMaster &lt;- ggplot(top_master) +\n  geom_bar(aes(x = reorder(ProvinceName, -MasterPercentage), y = MasterPercentage), \n           stat = \"identity\", fill = \"purple\", position = \"dodge\") + \n  labs(title = \"Distribution of the provinces with the highest percentage of master's degrees in 2020\",\n       x = \"Province\",\n       y = \"Percentage\") +\n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n        axis.text.y = element_text(size = 8))\n\ntopDoctorate &lt;- ggplot(top_doctorate) +\n  geom_bar(aes(x = reorder(ProvinceName, -DoctoratePercentage), y = DoctoratePercentage), \n           stat = \"identity\", fill = \"pink\", position = \"dodge\") + \n  labs(title = \"Distribution of the provinces with the highest percentage of Doctorate's degrees in 2020\",\n       x = \"Province\",\n       y = \"Percentage\") +\n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n        axis.text.y = element_text(size = 8))\n\n\nresult &lt;- inner_join(education2020, population, by = \"ProvinceName\")\n\nIlliterateResult &lt;- result %&gt;%\n  mutate(IlliteratePercentage = illiterateTotal / TotalPopulation * 100 )\n\ntop_illiterate &lt;- IlliterateResult %&gt;%\n  arrange(desc(IlliteratePercentage)) %&gt;%\n  head(10)\n\n\ntopIlliterate &lt;- ggplot(top_illiterate) +\n  geom_bar(aes(x = reorder(ProvinceName, -IlliteratePercentage), y = IlliteratePercentage), \n           stat = \"identity\", fill = \"blue\", position = \"dodge\") + \n  labs(title = \"Distribution of the provinces with the highest percentage of Illiterate people in Turkey in 2020\",\n       x = \"Province\",\n       y = \"Percentage\") +\n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n        axis.text.y = element_text(size = 8))\ngrid.arrange(topMaster, topDoctorate,topIlliterate, nrow = 3)\n\n\n\n\n\nIt is observed that the cities where people with doctorates and master’s degrees are concentrated are different from the cities where illiterate people are concentrated.In this context, it can be associated with different data sets and other analyzes can be made.\nBased on the above analysis results, analyzes will continue to be made with the prediction that it can be associated with a data set related to Turkey’s development level. In this context, the contribution data of the provinces to Türkiye’s gross national product will be used. Below, Turkey’s contribution rates to the gross national product by province for 2020 are examined. As can be seen, Ankara is among the provinces with the highest contribution. Ankara was also one of the provinces with the highest rates of doctoral and master’s degree holders. Based on this data, it comes to mind to examine whether there is a relationship between the high contribution to the gross national product in the provinces with high rates of doctoral and master’s degree holders.\n\n\nCode\nlibrary(ggrepel)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(devtools)\n\ndevtools::install_github(\"ropensci/rnaturalearthhires\")\n\n\nSkipping install of 'rnaturalearthhires' from a github remote, the SHA1 (dd1e210c) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nCode\nGSYMH2020 &lt;- GSYMH %&gt;% select(ProvinceNames, `2020`)\n\n\nturkeyChanged &lt;- turkeyChanged[complete.cases(turkeyChanged$featurecla),]\nturkeyGSYMH &lt;- inner_join(turkeyChanged, GSYMH2020, by = c(\"name\" = \"ProvinceNames\"))\n\nggplot(turkeyGSYMH) +\n  geom_sf(aes(fill = `2020`), color = \"black\") +\n  geom_sf_text(aes(label = ProvinceNames), size = 1.5, color = \"black\") +  \n  scale_fill_continuous(low = \"white\", high = \"blue\", name = \"GSYMH2020\") +\n  theme_minimal() +\n  labs(title = \"Turkey's Provinces by GSYMH2020\",\n       subtitle = \"GSYMH2020 values for illustration purposes\",\n       caption = \"Source: Generated data\") +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())\n\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nCode\ntop_GSYMH &lt;- GSYMH2020 %&gt;%\n  filter(`2020` &gt; 0) %&gt;%\n  arrange(desc(`2020`)) %&gt;%\n  head(10)\nprint(top_GSYMH)\n\n\n# A tibble: 10 × 2\n   ProvinceNames `2020`\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Ankara        0.587 \n 2 Gaziantep     0.210 \n 3 Kocaeli       0.187 \n 4 Mersin        0.127 \n 5 Konya         0.124 \n 6 Adana         0.114 \n 7 Izmir         0.109 \n 8 canakkale     0.0886\n 9 Tekirdag      0.0858\n10 Kayseri       0.0710\n\n\nCode\ntopGSYMH &lt;- ggplot(top_GSYMH) +\n  geom_bar(aes(x = reorder(ProvinceNames, -`2020`), y = `2020`), \n           stat = \"identity\", fill = \"blue\", position = \"dodge\") + \n  labs(title = \"Distribution of the provinces with the highest GDP of Turkey in 2020\",\n       x = \"Province\",\n       y = \"GDP in 2020\") +\n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 1),\n        axis.text.y = element_text(size = 8))\n\nprint(topGSYMH)\n\n\n\n\n\nThe ranking of the provinces that contribute the most to Turkey’s Gross National Product is given below.\n\n\nCode\ntop_GSYMH2020 &lt;- turkeyGSYMH %&gt;%\n  arrange(desc(`2020`))\n\ntop_GSYMH2020 &lt;- top_GSYMH2020 %&gt;%\n  select(`ProvinceNames`, `2020`)\nprint(top_GSYMH2020)\n\n\nSimple feature collection with 80 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25.66326 ymin: 35.81978 xmax: 44.80699 ymax: 42.09878\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   ProvinceNames       2020                       geometry\n1         Ankara 0.58719741 MULTIPOLYGON (((30.86357 40...\n2      Gaziantep 0.20996101 MULTIPOLYGON (((38.02601 36...\n3        Kocaeli 0.18701053 MULTIPOLYGON (((29.88461 41...\n4         Mersin 0.12683053 MULTIPOLYGON (((34.93328 36...\n5          Konya 0.12432588 MULTIPOLYGON (((31.86145 39...\n6          Adana 0.11374267 MULTIPOLYGON (((35.93252 36...\n7          Izmir 0.10941476 MULTIPOLYGON (((27.2618 37....\n8      canakkale 0.08856653 MULTIPOLYGON (((26.96354 40...\n9       Tekirdag 0.08580904 MULTIPOLYGON (((28.17473 41...\n10       Kayseri 0.07095621 MULTIPOLYGON (((35.19065 37...\n\n\nAs can be seen, Ankara ranks first with the highest GDP contribution. The city with the highest number of people with postgraduate education was Ankara. In this context, the contribution of the high number of people who have completed postgraduate education to the Gross National Product will be investigated.\nIn this context, we will get support from correlation to examine the relationship between these two situations.\n\n\nCode\ncorrelationTest &lt;- cor.test(top_doctorate$DoctoratePercentage, top_GSYMH$`2020`)\n\n\nprint(correlationTest)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  top_doctorate$DoctoratePercentage and top_GSYMH$`2020`\nt = 6.5417, df = 8, p-value = 0.00018\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6829344 0.9807243\nsample estimates:\n      cor \n0.9178788 \n\n\nCode\ncorrelation_plot &lt;- ggplot(data = NULL, aes(x = top_doctorate$DoctoratePercentage, y = top_GSYMH$`2020`)) +\n  geom_point(color = \"blue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Correlation Doctorate Percentage and GSYMH 2020\",\n       x = \" Doctorate Percentage\",\n       y = \"GSYMH 2020\") +\n  theme_minimal()\n\nprint(correlation_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe correlation coefficient took the value of 0.9178788, indicating that there is a very strong positive relationship between these two variables. The p-value is 0.00018, indicating that this correlation is statistically significant. The confidence interval is quite narrow, indicating the accuracy of the correlation. As a result, we can say that there is a very strong and statistically significant positive correlation between DoctoratePercentage and GDP contribution rate variables. This shows that GDP tends to increase as the percentage of PhDs increases.\n#3.3.2. Regression Analysis\n\n\nCode\nGSYMH2020$ProvinceName &lt;- GSYMH2020$ProvinceNames\nGSYMH2020 &lt;- GSYMH2020 %&gt;% filter(`2020` &gt; 0)\nregressionData &lt;- full_join(GSYMH2020, DoctorateResult, by = \"ProvinceName\")\n\nmodel &lt;- lm(`2020`~ DoctoratePercentage, data = regressionData)\n\nsummary(model)\n\n\n\nCall:\nlm(formula = `2020` ~ DoctoratePercentage, data = regressionData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10942 -0.03117 -0.01215  0.01128  0.41458 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -0.02460    0.01974  -1.246 0.217020    \nDoctoratePercentage  0.31138    0.08483   3.671 0.000481 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07143 on 67 degrees of freedom\n  (14 observations deleted due to missingness)\nMultiple R-squared:  0.1674,    Adjusted R-squared:  0.155 \nF-statistic: 13.47 on 1 and 67 DF,  p-value: 0.0004811\n\n\nCode\nplot(regressionData$DoctoratePercentage, regressionData$`2020`,\n     main = \"The relationship between the GDP percentage of provinces and the rate of people with a doctorate in 2020\",\n     xlab =\"Doctorate Percentage\" , ylab = \"GDP Percentage\", pch = 19, col = \"blue\")\nabline(model, col = \"red\")\n\n\n\n\n\nCode\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\nCode\nresiduals &lt;- resid(model)\nresiduals_squared &lt;- residuals^2\nsum_squared_residuals &lt;- sum(residuals_squared)\nn &lt;- length(residuals)\n\nmean_residual_variance &lt;- sum_squared_residuals / n\nprint(paste(\"Modelin hata terimlerinin varyansı:\", mean_residual_variance))\n\n\n[1] \"Modelin hata terimlerinin varyansı: 0.00495377108512356\"\n\n\nThe variance of the error terms turned out to be 0.015, and since the p value is less than 0.05, it can be interpreted that it does not have a constant variance. It took the value of p = 0.06927 and is not significant. The F-statistic is 3.395 and the p-value is 0.06927 This is used to evaluate whether the model is statistically significant in explaining the dependent variable. Since the p-value here is greater than 0.05, it cannot be said that the model is statistically significant. The R-squared value of the model is 0.03017 A low R-squared value indicates that the model does not have sufficient power to explain it.\nThe analysis here examines whether the gross national product data per capita in 2020 by provinces in Turkey has a relationship with the rates of doctoral graduates in the provinces in the same year.\n\n\nCode\nlibrary(dplyr)\n\nGDP2020 &lt;- GDP %&gt;% select(ProvinceNames, `2020`)\nGDP2020$ProvinceName &lt;- GDP2020$ProvinceNames\nregressionData2 &lt;- full_join(GDP2020, DoctorateResult, by = c(\"ProvinceName\"))\n\nmodel &lt;- lm(`2020`~ DoctoratePercentage, data = regressionData2)\n\nsummary(model)\n\n\n\nCall:\nlm(formula = `2020` ~ DoctoratePercentage, data = regressionData2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-21040  -8914  -1545   6452  52431 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            30364       3487   8.708 4.84e-13 ***\nDoctoratePercentage    74056      14910   4.967 4.08e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12810 on 76 degrees of freedom\n  (6 observations deleted due to missingness)\nMultiple R-squared:  0.2451,    Adjusted R-squared:  0.2351 \nF-statistic: 24.67 on 1 and 76 DF,  p-value: 4.078e-06\n\n\nCode\nplot(regressionData$DoctoratePercentage, regressionData$`2020`,\n     main = \"The relationship between the GDP per person of provinces and the rate of people with a doctorate in 2020\",\n     xlab =\"Doctorate Percentage\" , ylab = \"GDP Percentage\", pch = 19, col = \"blue\")\nabline(model, col = \"red\")\n\n\n\n\n\nCode\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\nCode\nresiduals &lt;- resid(model)\nresiduals_squared &lt;- residuals^2\nsum_squared_residuals &lt;- sum(residuals_squared)\nn &lt;- length(residuals)\n\nmean_residual_variance &lt;- sum_squared_residuals / n\nprint(paste(\"Modelin hata terimlerinin varyansı:\", mean_residual_variance))\n\n\n[1] \"Modelin hata terimlerinin varyansı: 159946237.57373\"\n\n\nThe slope took the value of 0.31138. Each time DoctoratePercentage increases by one unit, the GDP contribution rate variable increases by approximately 0.31138 units. This coefficient is highly significant (p-value &lt; 0.001). R-squared took the value of 0.1674. Approximately 16.74% of the variance in the GDP contribution rate variable is explained by DoctoratePercentage. This suggests that DoctoratePercentage is a significant predictor, but other factors not included in the model also greatly add to the variability. The adjusted R-squared had a value of 0.155. It is slightly lower than the R-squared value and makes the model have low explanatory power. As a result, it can be said that DoctoratePercentage is a significant predictor of the GDP contribution rate variable."
  },
  {
    "objectID": "project.html#results",
    "href": "project.html#results",
    "title": "Project Turkey’s Education Istatistics",
    "section": "3.4 Results",
    "text": "3.4 Results\nAs a result, data sets were examined using correlation, regression analysis and time series analysis. Interesting information was obtained by detecting significant significant relationships and, contrary to expectations, insignificant relationships. In the future, meaningful relationships and prediction studies can be developed by adding new data sets."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignment I conducted for the [term and year, e.g. Spring 2024] EMU660 Decision Making with Analytics course.\nPlease use left menu to navigate through my assignments.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nFirm Turkish Aerospace, position Production Planning Engineer, year 2023 - ongoing.\nFirm Arlight Lighting, position Production Planning Engineer, 2022 to 2023."
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nFirm Metro Gross Market, position Graduation Project, year 2021\nFirm MitaE\u001f, position Candidate Engineer, year 2021"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/assignments/assignment-1.html",
    "href": "docs/assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nMy first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "My first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Assignment 2\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Tap and reach MY CV\nI am highly motivated to progress professionally, improve my professional knowledge, and acquire new ones."
  },
  {
    "objectID": "assignments/assignment-1.html#b",
    "href": "assignments/assignment-1.html#b",
    "title": "Assignment 1",
    "section": "",
    "text": "library(dslabs) data(“mtcars”)\nCreated a custom_summary function and which is works to calculating mean, standard variance, max value in data set, min value in data set and median value.\n\ncustom_summary &lt;- function (carCustom)\n{\n    meanValue &lt;- mean(carCustom)\n    std &lt;- sd(carCustom)\n    maxValue &lt;- max(carCustom)\n    minValue &lt;- min(carCustom)\n    medianValue &lt;- median(carCustom)\n    customStatistics &lt;- list(minValue,maxValue,meanValue,medianValue,std)\n    return (customStatistics)\n}\n\nThis stage custom_summary fuction calculated all values (mean, standard variance, max value in data set, min value in data set and median value) for each property of mtcars.\n\ncustomNames &lt;- names(mtcars)\nfor (customIndex in 1:11){\n    carCustom &lt;- mtcars[,customIndex]\n    customStatistics &lt;- custom_summary(carCustom)\n    print(customNames[customIndex])\n    print(customStatistics)\n}\n\n[1] \"mpg\"\n[[1]]\n[1] 10.4\n\n[[2]]\n[1] 33.9\n\n[[3]]\n[1] 20.09062\n\n[[4]]\n[1] 19.2\n\n[[5]]\n[1] 6.026948\n\n[1] \"cyl\"\n[[1]]\n[1] 4\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 6.1875\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 1.785922\n\n[1] \"disp\"\n[[1]]\n[1] 71.1\n\n[[2]]\n[1] 472\n\n[[3]]\n[1] 230.7219\n\n[[4]]\n[1] 196.3\n\n[[5]]\n[1] 123.9387\n\n[1] \"hp\"\n[[1]]\n[1] 52\n\n[[2]]\n[1] 335\n\n[[3]]\n[1] 146.6875\n\n[[4]]\n[1] 123\n\n[[5]]\n[1] 68.56287\n\n[1] \"drat\"\n[[1]]\n[1] 2.76\n\n[[2]]\n[1] 4.93\n\n[[3]]\n[1] 3.596563\n\n[[4]]\n[1] 3.695\n\n[[5]]\n[1] 0.5346787\n\n[1] \"wt\"\n[[1]]\n[1] 1.513\n\n[[2]]\n[1] 5.424\n\n[[3]]\n[1] 3.21725\n\n[[4]]\n[1] 3.325\n\n[[5]]\n[1] 0.9784574\n\n[1] \"qsec\"\n[[1]]\n[1] 14.5\n\n[[2]]\n[1] 22.9\n\n[[3]]\n[1] 17.84875\n\n[[4]]\n[1] 17.71\n\n[[5]]\n[1] 1.786943\n\n[1] \"vs\"\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 0.4375\n\n[[4]]\n[1] 0\n\n[[5]]\n[1] 0.5040161\n\n[1] \"am\"\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 0.40625\n\n[[4]]\n[1] 0\n\n[[5]]\n[1] 0.4989909\n\n[1] \"gear\"\n[[1]]\n[1] 3\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 3.6875\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 0.7378041\n\n[1] \"carb\"\n[[1]]\n[1] 1\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 2.8125\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 1.6152\n\n\nThe apply function and the custom_summary function were applied to all columns of the matrix.\n\nqq &lt;- apply(mtcars,2,custom_summary)\nprint(qq)\n\n$mpg\n$mpg[[1]]\n[1] 10.4\n\n$mpg[[2]]\n[1] 33.9\n\n$mpg[[3]]\n[1] 20.09062\n\n$mpg[[4]]\n[1] 19.2\n\n$mpg[[5]]\n[1] 6.026948\n\n\n$cyl\n$cyl[[1]]\n[1] 4\n\n$cyl[[2]]\n[1] 8\n\n$cyl[[3]]\n[1] 6.1875\n\n$cyl[[4]]\n[1] 6\n\n$cyl[[5]]\n[1] 1.785922\n\n\n$disp\n$disp[[1]]\n[1] 71.1\n\n$disp[[2]]\n[1] 472\n\n$disp[[3]]\n[1] 230.7219\n\n$disp[[4]]\n[1] 196.3\n\n$disp[[5]]\n[1] 123.9387\n\n\n$hp\n$hp[[1]]\n[1] 52\n\n$hp[[2]]\n[1] 335\n\n$hp[[3]]\n[1] 146.6875\n\n$hp[[4]]\n[1] 123\n\n$hp[[5]]\n[1] 68.56287\n\n\n$drat\n$drat[[1]]\n[1] 2.76\n\n$drat[[2]]\n[1] 4.93\n\n$drat[[3]]\n[1] 3.596563\n\n$drat[[4]]\n[1] 3.695\n\n$drat[[5]]\n[1] 0.5346787\n\n\n$wt\n$wt[[1]]\n[1] 1.513\n\n$wt[[2]]\n[1] 5.424\n\n$wt[[3]]\n[1] 3.21725\n\n$wt[[4]]\n[1] 3.325\n\n$wt[[5]]\n[1] 0.9784574\n\n\n$qsec\n$qsec[[1]]\n[1] 14.5\n\n$qsec[[2]]\n[1] 22.9\n\n$qsec[[3]]\n[1] 17.84875\n\n$qsec[[4]]\n[1] 17.71\n\n$qsec[[5]]\n[1] 1.786943\n\n\n$vs\n$vs[[1]]\n[1] 0\n\n$vs[[2]]\n[1] 1\n\n$vs[[3]]\n[1] 0.4375\n\n$vs[[4]]\n[1] 0\n\n$vs[[5]]\n[1] 0.5040161\n\n\n$am\n$am[[1]]\n[1] 0\n\n$am[[2]]\n[1] 1\n\n$am[[3]]\n[1] 0.40625\n\n$am[[4]]\n[1] 0\n\n$am[[5]]\n[1] 0.4989909\n\n\n$gear\n$gear[[1]]\n[1] 3\n\n$gear[[2]]\n[1] 5\n\n$gear[[3]]\n[1] 3.6875\n\n$gear[[4]]\n[1] 4\n\n$gear[[5]]\n[1] 0.7378041\n\n\n$carb\n$carb[[1]]\n[1] 1\n\n$carb[[2]]\n[1] 8\n\n$carb[[3]]\n[1] 2.8125\n\n$carb[[4]]\n[1] 2\n\n$carb[[5]]\n[1] 1.6152"
  },
  {
    "objectID": "assignments/assignment-1.html#c",
    "href": "assignments/assignment-1.html#c",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)\nThere is a Data Graph belongs to na_example data set.\n\nlibrary(dslabs)\ndata(\"na_example\")\nplot(na_example, type = \"l\", main = \"Data Graph\", xlab = \"Example Number\", ylab = \"Example Value\")\n\n\n\n\nYou can see all the values of the na_example data set in the table below.\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nNumber of elements which equal to NA in Data Set\n\nsum(is.na(na_example))\n\n[1] 145\n\n\nConvert NA’s to 660\n\nconvert_nas &lt;- ifelse(is.na(na_example),660, na_example) \n\nYou can see converted data below.\n\nprint(convert_nas)\n\n   [1]   2   1   3   2   1   3   1   4   3   2   2 660   2   2   1   4 660   1\n  [19]   1   2   1   2   2   1   2   5 660   2   2   3   1   2   4   1   1   1\n  [37]   4   5   2   3   4   1   2   4   1   1   2   1   5 660 660 660   1   1\n  [55]   5   1   3   1 660   4   4   7   3   2 660 660   1 660   4   1   2   2\n  [73]   3   2   1   2   2   4   3   4   2   3   1   3   2   1   1   1   3   1\n  [91] 660   3   1   2   2   1   2   2   1   1   4   1   1   2   3   3   2   2\n [109]   3   3   3   4   1   1   1   2 660   4   3   4   3   1   2   1 660 660\n [127] 660 660   1   5   1   2   1   3   5   3   2   2 660 660 660 660   3   5\n [145]   3   1   1   4   2   4   3   3 660   2   3   2   6 660   1   1   2   2\n [163]   1   3   1   1   5 660 660   2   4 660   2   5   1   4   3   3 660   4\n [181]   3   1   4   1   1   3   1   1 660 660   3   5   2   2   2   3   1   2\n [199]   2   3   2   1 660   2 660   1 660 660   2   1   1 660   3 660   1   2\n [217]   2   1   3   2   2   1   1   2   3   1   1   1   4   3   4   2   2   1\n [235]   4   1 660   5   1   4 660   3 660 660   1   1   5   2   3   3   2   4\n [253] 660   3   2   5 660   2   3   4   6   2   2   2 660   2 660   2 660   3\n [271]   3   2   2   4   3   1   4   2 660   2   4 660   6   2   3   1 660   2\n [289]   2 660   1   1   3   2   3   3   1 660   1   4   2   1   1   3   2   1\n [307]   2   3   1 660   2   3   3   2   1   2   3   5   5   1   2   3   3   1\n [325] 660 660   1   2   4 660   2   1   1   1   3   2   1   1   3   4 660   1\n [343]   2   1   1   3   3 660   1   1   3   5   3   2   3   4   1   4   3   1\n [361] 660   2   1   2   2   1   2   2   6   1   2   4   5 660   3   4   2   1\n [379]   1   4   2   1   1   1   1   2   1   4   4   1   3 660   3   3 660   2\n [397] 660   1   2   1   1   4   2   1   4   4 660   1   2 660   3   2   2   2\n [415]   1   4   3   6   1   2   3   1   3   2   2   2   1   1   3   2   1   1\n [433]   1   3   2   2 660   4   4   4   1   1 660   4   3 660   1   3   1   3\n [451]   2   4   2   2   2   3   2   1   4   3 660   1   4   3   1   3   2 660\n [469]   3 660   1   3   1   4   1   1   1   2   4   3   1   2   2   2   3   2\n [487]   3   1   1 660   3   2   1   1   2 660   2   2   2   3   3   1   1   2\n [505] 660   1   2   1   1   3   3   1   3   1   1   1   1   1   2   5   1   1\n [523]   2   2   1   1 660   1   4   1   2   4   1   3   2 660   1   1 660   2\n [541]   1   1   4   2   3   3   1   5   3   1   1   2 660   1   1   3   1   3\n [559]   2   4 660   2   3   2   1   2   1   1   1   2   2   3   1   5   2 660\n [577]   2 660   3   2   2   2   1   5   3   2   3   1 660   3   1   2   2   2\n [595]   1   2   2   4 660   6   1   2 660   1   1   2   2   3 660   3   2   3\n [613]   3   4   2 660   2 660   4 660   1   1   2   2   3   1   1   1   3 660\n [631]   2   5 660   7   1 660   4   3   3   1 660   1   1   1   1   3   2   4\n [649]   2   2   3 660 660   1   4   3   2   2   2   3   2   4   2   2   4 660\n [667] 660 660   6   3   3   1   4   4   2   1 660   1   6 660   3   3   2   1\n [685]   1   6 660   1   5   1 660   2   6   2 660   4   1   3   1   2 660   1\n [703]   1   3   1   2   4   2   1   3   2   4   3   2   2   1   1   5   6   4\n [721]   2   2   2   2   4 660   1   2   2   2   2   4   5 660 660 660   4   3\n [739]   3   3   2   4   2   4 660 660 660 660   2   1 660   2   4   3   2 660\n [757]   2   3   1   3   4 660   1   2   1   2 660   3   1   2   1   2   1   2\n [775]   1   2   2   2   2   1   1   3   3   1   3   4   3 660 660   4   2   3\n [793]   2   1   3   2   4   2   2   3   1   2   4   3   3   4 660   1   4   2\n [811]   1   1   1   3   1   5   2   2   4   2 660   1   3   1   2 660   1   2\n [829]   1   2   1 660   1   3   2   3   2 660   2   1   4   2 660 660 660   2\n [847]   4   2 660 660   3   1 660   5   5   2   2   2 660   2   1   3   1   3\n [865]   2   4   2   4 660   4   1   2   3   2   3   3   2   3   2   2   2   1\n [883]   3   2   4   2 660   3   3   2   2 660 660   3   2   1   2   4   1   1\n [901]   1   1   4   3   2 660   3   2 660   1 660   3   2   1   1   1   2 660\n [919]   2   2   3   3   2 660 660   4   5   2   2   2   1   2   3   1   3   3\n [937]   4   3 660   1   1   1 660   4   3   5   1   1   2 660   2   2   2   2\n [955]   5   2   2   3   1   2   3 660   1   2 660 660   2 660   3   1   1   2\n [973]   5   3   5   1   1   4 660   2   1   3   1   1   2   4   3   3   3 660\n [991]   1   1   2   2   1   1   2   2 660   2\n\n\nTotal NA values at updated data set.\n\nsum(is.na(convert_nas))\n\n[1] 0\n\n\nNumber of NA elements which are assigned to 660.\n\nsumpro&lt;-sum(convert_nas==660)\nprint(sumpro)\n\n[1] 145"
  },
  {
    "objectID": "assignments/assignment-1.html#a-conversations-on-data-science-and-industrial-engineering---mustafa-baydoan-erdi-dademir",
    "href": "assignments/assignment-1.html#a-conversations-on-data-science-and-industrial-engineering---mustafa-baydoan-erdi-dademir",
    "title": "Assignment 1",
    "section": "",
    "text": "During an interview on Data Science and Industrial Engineering, Mustafa Baydo??an shared valuable insights on how to solve real-life problems with data science. To start, let me briefly introduce Mustafa Baydo??an. He is an instructor at Bo??azi??i University’s Industrial Engineering department and also the founder of Algopoly, a software and consultancy company located in Istanbul. His research focuses on using data science tools and techniques for large-scale data mining, time series analysis, pattern discovery, and operations research. He received his undergraduate and graduate degrees from METU and his doctorate from Arizona State University. In his Master’s degree, he worked on multi-criteria decision-making and later shifted his focus to data science in his doctoral studies, where he worked on topics such as time series analysis, forecasting and creating solutions with machine learning methods. After gaining experience in consultancy and private sector, he founded Algopoly in 2017, where the company primarily serves the energy and logistics sectors.\nDuring the interview, Mustafa Baydo??an discussed several real-life problems that data science. For instance, he mentioned a several company demand forecast study is being carry out for shoe sales. He mentions that forecasting applications should be used to solve this problem. He also shared an example of a company in the USA that owns forests used for timber production. After the drying process, some timbers become warped. It is essential to estimate how much of these timbers will be bent as the value of timber in the market varies depending on its shape. By using image processing and machine learning methods, data can be collected by merging the images and processing the information to create a dataset. With this solution, the company can identify the timbers that are likely to become bent before the drying process, and corrective actions can be taken at a low cost. According to Mustafa Baydo??an, this can increase profits by 5 percent.\nFurthermore, he also talked about energy consumption and production balance. Forecasting is frequently used to ensure balance in electricity markets and establish the balance between production and consumption. If the forecast does not come true, an imbalance occurs, and companies incur costs. To prevent this, estimates can be made by calculating the production amount based on regional consumption data and taking into account special situations.\nIn addition, Mustafa Baydo??an shared an example of Trendyol, another data processing company that uses models to make predictions based on the number of units a product will sell and whether anyone has looked at the product before. In Trendyol’s model, penalty coefficients are assigned to products that are frequently viewed but not purchased, thus preventing them from appearing in front of the user. By doing this, Trendyol can provide its users with a better shopping experience by eliminating irrelevant options.\nOverall, Mustafa Baydo??an’s insights were valuable and enlightening, providing a glimpse into how data science can be used to solve real-life problems from different perspectives."
  },
  {
    "objectID": "assignments/assignment-1.html#a-conversations-on-data-science-and-industrial-engineering---mustafa-baydogan-erdi-dasdemir",
    "href": "assignments/assignment-1.html#a-conversations-on-data-science-and-industrial-engineering---mustafa-baydogan-erdi-dasdemir",
    "title": "Assignment 1",
    "section": "(a) Conversations on Data Science and Industrial Engineering - Mustafa Baydogan, Erdi Dasdemir",
    "text": "(a) Conversations on Data Science and Industrial Engineering - Mustafa Baydogan, Erdi Dasdemir\nDuring an interview on Data Science and Industrial Engineering, Mustafa Baydogan shared valuable insights on how to solve real-life problems with data science. To start, let me briefly introduce Mustafa Baydogan. He is an instructor at Bogazici University’s Industrial Engineering department and also the founder of Algopoly, a software and consultancy company located in Istanbul. His research focuses on using data science tools and techniques for large-scale data mining, time series analysis, pattern discovery, and operations research. He received his undergraduate and graduate degrees from METU and his doctorate from Arizona State University. In his Master’s degree, he worked on multi-criteria decision-making and later shifted his focus to data science in his doctoral studies, where he worked on topics such as time series analysis, forecasting and creating solutions with machine learning methods. After gaining experience in consultancy and private sector, he founded Algopoly in 2017, where the company primarily serves the energy and logistics sectors.\nDuring the interview, Mustafa Baydogan discussed several real-life problems that data science. For instance, he mentioned a several company demand forecast study is being carry out for shoe sales. He mentions that forecasting applications should be used to solve this problem. He also shared an example of a company in the USA that owns forests used for timber production. After the drying process, some timbers become warped. It is essential to estimate how much of these timbers will be bent as the value of timber in the market varies depending on its shape. By using image processing and machine learning methods, data can be collected by merging the images and processing the information to create a dataset. With this solution, the company can identify the timbers that are likely to become bent before the drying process, and corrective actions can be taken at a low cost. According to Mustafa Baydogan, this can increase profits by 5 percent.\nFurthermore, he also talked about energy consumption and production balance. Forecasting is frequently used to ensure balance in electricity markets and establish the balance between production and consumption. If the forecast does not come true, an imbalance occurs, and companies incur costs. To prevent this, estimates can be made by calculating the production amount based on regional consumption data and taking into account special situations.\nIn addition, Mustafa Baydogan shared an example of Trendyol, another data processing company that uses models to make predictions based on the number of units a product will sell and whether anyone has looked at the product before. In Trendyols model, penalty coefficients are assigned to products that are frequently viewed but not purchased, thus preventing them from appearing in front of the user. By doing this, Trendyol can provide its users with a better shopping experience by eliminating irrelevant options.\nOverall, Mustafa Baydogan’s insights were valuable and enlightening, providing a glimpse into how data science can be used to solve real-life problems from different perspectives."
  },
  {
    "objectID": "assignments/assignment-1.html#b-created-a-custom_summary-function-and-which-is-works-to-calculating-mean-standard-variance-max-value-in-data-set-min-value-in-data-set-and-median-value.",
    "href": "assignments/assignment-1.html#b-created-a-custom_summary-function-and-which-is-works-to-calculating-mean-standard-variance-max-value-in-data-set-min-value-in-data-set-and-median-value.",
    "title": "Assignment 1",
    "section": "(b) Created a custom_summary function and which is works to calculating mean, standard variance, max value in data set, min value in data set and median value.",
    "text": "(b) Created a custom_summary function and which is works to calculating mean, standard variance, max value in data set, min value in data set and median value.\n\ncustom_summary &lt;- function (carCustom)\n{\n    meanValue &lt;- mean(carCustom)\n    std &lt;- sd(carCustom)\n    maxValue &lt;- max(carCustom)\n    minValue &lt;- min(carCustom)\n    medianValue &lt;- median(carCustom)\n    customStatistics &lt;- list(minValue,maxValue,meanValue,medianValue,std)\n    return (customStatistics)\n}\n\nThis stage custom_summary fuction calculated all values (mean, standard variance, max value in data set, min value in data set and median value) for each property of mtcars.\n\ncustomNames &lt;- names(mtcars)\nfor (customIndex in 1:11){\n    carCustom &lt;- mtcars[,customIndex]\n    customStatistics &lt;- custom_summary(carCustom)\n    print(customNames[customIndex])\n    print(customStatistics)\n}\n\n[1] \"mpg\"\n[[1]]\n[1] 10.4\n\n[[2]]\n[1] 33.9\n\n[[3]]\n[1] 20.09062\n\n[[4]]\n[1] 19.2\n\n[[5]]\n[1] 6.026948\n\n[1] \"cyl\"\n[[1]]\n[1] 4\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 6.1875\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 1.785922\n\n[1] \"disp\"\n[[1]]\n[1] 71.1\n\n[[2]]\n[1] 472\n\n[[3]]\n[1] 230.7219\n\n[[4]]\n[1] 196.3\n\n[[5]]\n[1] 123.9387\n\n[1] \"hp\"\n[[1]]\n[1] 52\n\n[[2]]\n[1] 335\n\n[[3]]\n[1] 146.6875\n\n[[4]]\n[1] 123\n\n[[5]]\n[1] 68.56287\n\n[1] \"drat\"\n[[1]]\n[1] 2.76\n\n[[2]]\n[1] 4.93\n\n[[3]]\n[1] 3.596563\n\n[[4]]\n[1] 3.695\n\n[[5]]\n[1] 0.5346787\n\n[1] \"wt\"\n[[1]]\n[1] 1.513\n\n[[2]]\n[1] 5.424\n\n[[3]]\n[1] 3.21725\n\n[[4]]\n[1] 3.325\n\n[[5]]\n[1] 0.9784574\n\n[1] \"qsec\"\n[[1]]\n[1] 14.5\n\n[[2]]\n[1] 22.9\n\n[[3]]\n[1] 17.84875\n\n[[4]]\n[1] 17.71\n\n[[5]]\n[1] 1.786943\n\n[1] \"vs\"\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 0.4375\n\n[[4]]\n[1] 0\n\n[[5]]\n[1] 0.5040161\n\n[1] \"am\"\n[[1]]\n[1] 0\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 0.40625\n\n[[4]]\n[1] 0\n\n[[5]]\n[1] 0.4989909\n\n[1] \"gear\"\n[[1]]\n[1] 3\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 3.6875\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 0.7378041\n\n[1] \"carb\"\n[[1]]\n[1] 1\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 2.8125\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 1.6152\n\n\nThe apply function and the custom_summary function were applied to all columns of the matrix.\n\nqq &lt;- apply(mtcars,2,custom_summary)\nprint(qq)\n\n$mpg\n$mpg[[1]]\n[1] 10.4\n\n$mpg[[2]]\n[1] 33.9\n\n$mpg[[3]]\n[1] 20.09062\n\n$mpg[[4]]\n[1] 19.2\n\n$mpg[[5]]\n[1] 6.026948\n\n\n$cyl\n$cyl[[1]]\n[1] 4\n\n$cyl[[2]]\n[1] 8\n\n$cyl[[3]]\n[1] 6.1875\n\n$cyl[[4]]\n[1] 6\n\n$cyl[[5]]\n[1] 1.785922\n\n\n$disp\n$disp[[1]]\n[1] 71.1\n\n$disp[[2]]\n[1] 472\n\n$disp[[3]]\n[1] 230.7219\n\n$disp[[4]]\n[1] 196.3\n\n$disp[[5]]\n[1] 123.9387\n\n\n$hp\n$hp[[1]]\n[1] 52\n\n$hp[[2]]\n[1] 335\n\n$hp[[3]]\n[1] 146.6875\n\n$hp[[4]]\n[1] 123\n\n$hp[[5]]\n[1] 68.56287\n\n\n$drat\n$drat[[1]]\n[1] 2.76\n\n$drat[[2]]\n[1] 4.93\n\n$drat[[3]]\n[1] 3.596563\n\n$drat[[4]]\n[1] 3.695\n\n$drat[[5]]\n[1] 0.5346787\n\n\n$wt\n$wt[[1]]\n[1] 1.513\n\n$wt[[2]]\n[1] 5.424\n\n$wt[[3]]\n[1] 3.21725\n\n$wt[[4]]\n[1] 3.325\n\n$wt[[5]]\n[1] 0.9784574\n\n\n$qsec\n$qsec[[1]]\n[1] 14.5\n\n$qsec[[2]]\n[1] 22.9\n\n$qsec[[3]]\n[1] 17.84875\n\n$qsec[[4]]\n[1] 17.71\n\n$qsec[[5]]\n[1] 1.786943\n\n\n$vs\n$vs[[1]]\n[1] 0\n\n$vs[[2]]\n[1] 1\n\n$vs[[3]]\n[1] 0.4375\n\n$vs[[4]]\n[1] 0\n\n$vs[[5]]\n[1] 0.5040161\n\n\n$am\n$am[[1]]\n[1] 0\n\n$am[[2]]\n[1] 1\n\n$am[[3]]\n[1] 0.40625\n\n$am[[4]]\n[1] 0\n\n$am[[5]]\n[1] 0.4989909\n\n\n$gear\n$gear[[1]]\n[1] 3\n\n$gear[[2]]\n[1] 5\n\n$gear[[3]]\n[1] 3.6875\n\n$gear[[4]]\n[1] 4\n\n$gear[[5]]\n[1] 0.7378041\n\n\n$carb\n$carb[[1]]\n[1] 1\n\n$carb[[2]]\n[1] 8\n\n$carb[[3]]\n[1] 2.8125\n\n$carb[[4]]\n[1] 2\n\n$carb[[5]]\n[1] 1.6152"
  },
  {
    "objectID": "assignments/assignment-1.html#c-na-example",
    "href": "assignments/assignment-1.html#c-na-example",
    "title": "Assignment 1",
    "section": "(c) NA Example",
    "text": "(c) NA Example\nThere is a Data Graph belongs to na_example data set.\n\nlibrary(dslabs)\ndata(\"na_example\")\nplot(na_example, type = \"l\", main = \"Data Graph\", xlab = \"Example Number\", ylab = \"Example Value\")\n\n\n\n\nYou can see all the values of the na_example data set in the table below.\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nNumber of elements which equal to NA in Data Set\n\nsum(is.na(na_example))\n\n[1] 145\n\n\nConvert NA’s to 660\n\nconvert_nas &lt;- ifelse(is.na(na_example),660, na_example) \n\nYou can see converted data below.\n\nprint(convert_nas)\n\n   [1]   2   1   3   2   1   3   1   4   3   2   2 660   2   2   1   4 660   1\n  [19]   1   2   1   2   2   1   2   5 660   2   2   3   1   2   4   1   1   1\n  [37]   4   5   2   3   4   1   2   4   1   1   2   1   5 660 660 660   1   1\n  [55]   5   1   3   1 660   4   4   7   3   2 660 660   1 660   4   1   2   2\n  [73]   3   2   1   2   2   4   3   4   2   3   1   3   2   1   1   1   3   1\n  [91] 660   3   1   2   2   1   2   2   1   1   4   1   1   2   3   3   2   2\n [109]   3   3   3   4   1   1   1   2 660   4   3   4   3   1   2   1 660 660\n [127] 660 660   1   5   1   2   1   3   5   3   2   2 660 660 660 660   3   5\n [145]   3   1   1   4   2   4   3   3 660   2   3   2   6 660   1   1   2   2\n [163]   1   3   1   1   5 660 660   2   4 660   2   5   1   4   3   3 660   4\n [181]   3   1   4   1   1   3   1   1 660 660   3   5   2   2   2   3   1   2\n [199]   2   3   2   1 660   2 660   1 660 660   2   1   1 660   3 660   1   2\n [217]   2   1   3   2   2   1   1   2   3   1   1   1   4   3   4   2   2   1\n [235]   4   1 660   5   1   4 660   3 660 660   1   1   5   2   3   3   2   4\n [253] 660   3   2   5 660   2   3   4   6   2   2   2 660   2 660   2 660   3\n [271]   3   2   2   4   3   1   4   2 660   2   4 660   6   2   3   1 660   2\n [289]   2 660   1   1   3   2   3   3   1 660   1   4   2   1   1   3   2   1\n [307]   2   3   1 660   2   3   3   2   1   2   3   5   5   1   2   3   3   1\n [325] 660 660   1   2   4 660   2   1   1   1   3   2   1   1   3   4 660   1\n [343]   2   1   1   3   3 660   1   1   3   5   3   2   3   4   1   4   3   1\n [361] 660   2   1   2   2   1   2   2   6   1   2   4   5 660   3   4   2   1\n [379]   1   4   2   1   1   1   1   2   1   4   4   1   3 660   3   3 660   2\n [397] 660   1   2   1   1   4   2   1   4   4 660   1   2 660   3   2   2   2\n [415]   1   4   3   6   1   2   3   1   3   2   2   2   1   1   3   2   1   1\n [433]   1   3   2   2 660   4   4   4   1   1 660   4   3 660   1   3   1   3\n [451]   2   4   2   2   2   3   2   1   4   3 660   1   4   3   1   3   2 660\n [469]   3 660   1   3   1   4   1   1   1   2   4   3   1   2   2   2   3   2\n [487]   3   1   1 660   3   2   1   1   2 660   2   2   2   3   3   1   1   2\n [505] 660   1   2   1   1   3   3   1   3   1   1   1   1   1   2   5   1   1\n [523]   2   2   1   1 660   1   4   1   2   4   1   3   2 660   1   1 660   2\n [541]   1   1   4   2   3   3   1   5   3   1   1   2 660   1   1   3   1   3\n [559]   2   4 660   2   3   2   1   2   1   1   1   2   2   3   1   5   2 660\n [577]   2 660   3   2   2   2   1   5   3   2   3   1 660   3   1   2   2   2\n [595]   1   2   2   4 660   6   1   2 660   1   1   2   2   3 660   3   2   3\n [613]   3   4   2 660   2 660   4 660   1   1   2   2   3   1   1   1   3 660\n [631]   2   5 660   7   1 660   4   3   3   1 660   1   1   1   1   3   2   4\n [649]   2   2   3 660 660   1   4   3   2   2   2   3   2   4   2   2   4 660\n [667] 660 660   6   3   3   1   4   4   2   1 660   1   6 660   3   3   2   1\n [685]   1   6 660   1   5   1 660   2   6   2 660   4   1   3   1   2 660   1\n [703]   1   3   1   2   4   2   1   3   2   4   3   2   2   1   1   5   6   4\n [721]   2   2   2   2   4 660   1   2   2   2   2   4   5 660 660 660   4   3\n [739]   3   3   2   4   2   4 660 660 660 660   2   1 660   2   4   3   2 660\n [757]   2   3   1   3   4 660   1   2   1   2 660   3   1   2   1   2   1   2\n [775]   1   2   2   2   2   1   1   3   3   1   3   4   3 660 660   4   2   3\n [793]   2   1   3   2   4   2   2   3   1   2   4   3   3   4 660   1   4   2\n [811]   1   1   1   3   1   5   2   2   4   2 660   1   3   1   2 660   1   2\n [829]   1   2   1 660   1   3   2   3   2 660   2   1   4   2 660 660 660   2\n [847]   4   2 660 660   3   1 660   5   5   2   2   2 660   2   1   3   1   3\n [865]   2   4   2   4 660   4   1   2   3   2   3   3   2   3   2   2   2   1\n [883]   3   2   4   2 660   3   3   2   2 660 660   3   2   1   2   4   1   1\n [901]   1   1   4   3   2 660   3   2 660   1 660   3   2   1   1   1   2 660\n [919]   2   2   3   3   2 660 660   4   5   2   2   2   1   2   3   1   3   3\n [937]   4   3 660   1   1   1 660   4   3   5   1   1   2 660   2   2   2   2\n [955]   5   2   2   3   1   2   3 660   1   2 660 660   2 660   3   1   1   2\n [973]   5   3   5   1   1   4 660   2   1   3   1   1   2   4   3   3   3 660\n [991]   1   1   2   2   1   1   2   2 660   2\n\n\nTotal NA values at updated data set.\n\nsum(is.na(convert_nas))\n\n[1] 0\n\n\nNumber of NA elements which are assigned to 660.\n\nsumpro&lt;-sum(convert_nas==660)\nprint(sumpro)\n\n[1] 145"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! My name is Ecem Sena Unlu\nThis is my personal webpage..\nPlease stay tuned to follow my works on data analytics, blog posts, and more."
  }
]